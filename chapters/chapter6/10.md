<!-- DISABLE-FRONTMATTER-SECTIONS -->

# End-of-chapter quiz



Let's test what you learned in this chapter!

### 1. When should you train a new tokenizer?

A. When the existing tokenizer is too slow
B. When you have a new language
C. When the vocabulary is too large
D. When the model is overfitting

<details><summary>Reveal Answer</summary>B</details>

### 2. What is the advantage of using a generator of lists of texts compared to a list of lists of texts when using `train_new_from_iterator()`?

A. It is faster
B. It uses less memory
C. It is more accurate
D. It is easier to implement

<details><summary>Reveal Answer</summary>B</details>

### 3. What are the advantages of using a "fast" tokenizer?

A. It is more accurate
B. It is easier to use
C. It is faster and uses less memory
D. It supports more languages

<details><summary>Reveal Answer</summary>C</details>

### 4. How does the `token-classification` pipeline handle entities that span over several tokens?

A. It merges them into one
B. It splits them into separate entities
C. It ignores them
D. It marks them as invalid

<details><summary>Reveal Answer</summary>A</details>

### 5. How does the `question-answering` pipeline handle long contexts?

A. It truncates them
B. It splits them into smaller parts
C. It processes them as a whole
D. It ignores them

<details><summary>Reveal Answer</summary>B</details>

### 6. What is normalization?

A. Converting text to lowercase
B. Removing punctuation
C. Standardizing text format
D. All of the above

<details><summary>Reveal Answer</summary>D</details>

### 7. What is pre-tokenization for a subword tokenizer?

A. Splitting text into sentences
B. Splitting text into words
C. Splitting text into characters
D. Splitting text into subwords

<details><summary>Reveal Answer</summary>B</details>

### 8. Select the sentences that apply to the BPE model of tokenization.

A. It uses a fixed vocabulary
B. It is based on word frequency
C. It merges frequent pairs of characters
D. It is language-specific

<details><summary>Reveal Answer</summary>C</details>

### 9. Select the sentences that apply to the WordPiece model of tokenization.

A. It uses a fixed vocabulary
B. It is based on word frequency
C. It merges frequent pairs of characters
D. It is language-specific

<details><summary>Reveal Answer</summary>A</details>

### 10. Select the sentences that apply to the Unigram model of tokenization.

A. It uses a fixed vocabulary
B. It is based on word frequency
C. It merges frequent pairs of characters
D. It is language-specific

<details><summary>Reveal Answer</summary>B</details>
