<!-- DISABLE-FRONTMATTER-SECTIONS -->

# End-of-chapter quiz



This chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood.

First, though, let's test what you learned in this chapter!


1.What will the following code return?
```
from transformers import pipeline
ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

A. Classification scores like "positive" or "negative"

B. A generated continuation of the sentence

C. Named entities like persons, organizations, or locations

D. Syntax parse trees

<details> <summary>Reveal Answer</summary> C. Named entities like persons, organizations, or locations  </details>

<br>


2.What should replace ... in this code?
```
from transformers import pipeline
filler = pipeline("fill-mask", model="bert-base-cased")
result = filler("...")
```
A. This <mask> has been waiting for you.

B. This [MASK] has been waiting for you.

C. This man has been waiting for you.

D. This mask has been you waiting for.

<details> <summary>Reveal Answer</summary> B. This [MASK] has been waiting for you. </details>

<br>



3.Why will this code fail?
```
from transformers import pipeline
classifier = pipeline("zero-shot-classification")
result = classifier("This is a course about the Transformers library")
```

A. This pipeline requires that labels be given to classify this text.

B. It requires multiple sentences

C. Transformers library is broken

D. The input text is too short

<details> <summary>Reveal Answer</summary> A. This pipeline requires that labels be given to classify this text.</details>

<br>


4.What does "transfer learning" mean?

A.  Transferring the knowledge of a pretrained model to a new model by training it on the same dataset.

B.  Transferring the knowledge of a pretrained model to a new model by building the second model with the same architecture as the first model.

C.  Transferring the knowledge of a pretrained model to a new model by initializing the second model with the first model's weights.

D. Pretraining a model twice

<details> <summary>Reveal Answer</summary> C.  Transferring the knowledge of a pretrained model to a new model by initializing the second model with the first model's weights.  </details>

<br>

5.True or false? A language model usually does not need labels for its pretraining.

A. True

B. False

<details> <summary>Reveal Answer</summary> A. True  </details>

<br>

6.Which statement best describes model, architecture, and weights?

A.  An architecture is a succession of mathematical functions to build a model and its weights are those functions parameters.

B. Architecture is a map; weights are cities

C.  If a model is a building, its architecture is the blueprint and the weights are the people living inside.

D. Weights describe the dataset used to build the model

<details> <summary>Reveal Answer</summary> An architecture is a succession of mathematical functions to build a model and its weights are those functions parameters.  </details>

<br>

7.Which type of model is best for generating text from prompts?

A. Encoder

B. Decoder 

C. Sequence-to-sequence

D. Hybrid

<details> <summary>Reveal Answer</summary> B. Decoder  </details>

<br>


8.Which model is best suited for summarization?

A. Encoder

B. Decoder

C. Sequence-to-sequence

D. Clustering model

<details> <summary>Reveal Answer</summary> C. Sequence-to-sequence</details>

<br>

9.Which model is best for text classification tasks?

A. Encoder

B. Decoder

C. Sequence-to-sequence

D. Reinforcement model

<details> <summary>Reveal Answer</summary> A. Encoder </details>

<br>


10.What is a possible source of bias in a model? (multiple correct answers)
A. The model is a fine-tuned version of a pretrained model and it picked up its bias from it.

B. The data the model was trained on is biased.

C. The metric the model was optimizing for is biased.

D. The model architecture itself is biased.

<details> <summary>Reveal Answer</summary> All answers </details>

<br>



