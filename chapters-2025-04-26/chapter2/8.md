<!-- DISABLE-FRONTMATTER-SECTIONS -->

# End-of-chapter quiz

1. What is the order of the language modeling pipeline?
A) First, the model, which handles text and returns raw predictions. The tokenizer then makes sense of these predictions and converts them back to text when needed.
B) First, the tokenizer, which handles text and returns IDs. The model handles these IDs and outputs a prediction, which can be some text.
C) The tokenizer handles text and returns IDs. The model handles these IDs and outputs a prediction. The tokenizer can then be used once again to convert these predictions back to some text.
<details><summary>Reveal Answer</summary> C. The tokenizer is responsible for both tokenizing input text into IDs and detokenizing model predictions back to text. </details>

2. How many dimensions does the tensor output by the base Transformer model have, and what are they?
A) 2: The sequence length and the batch size
B) 2: The sequence length and the hidden size
C) 3: The sequence length, the batch size, and the hidden size
<details><summary>Reveal Answer</summary> C. The correct dimensions are sequence length × batch size × hidden size. </details>

3. Which of the following is an example of subword tokenization? (multiple)
A) WordPiece
B) Character-based tokenization
C) Splitting on whitespace and punctuation
D) BPE (Byte-Pair Encoding)
E) Unigram
F) None of the above
<details><summary>Reveal Answer</summary> A, D, E. WordPiece, BPE, and Unigram are examples of subword tokenization. </details>

4. What is a model head?
A) A component of the base Transformer network that redirects tensors to their correct layers
B) Also known as the self-attention mechanism, it adapts the representation of a token according to the other tokens of the sequence
C) An additional component, usually made up of one or a few layers, to convert the transformer predictions to a task-specific output
<details><summary>Reveal Answer</summary> C. Model heads convert general-purpose Transformer outputs into task-specific outputs (e.g., classification, question answering). </details>

5. What is an AutoModel (or TFAutoModel)?
A) A model that automatically trains on your data
B) An object that returns the correct architecture based on the checkpoint
C) A model that automatically detects the language used for its inputs to load the correct weights
<details><summary>Reveal Answer</summary> B. AutoModel or TFAutoModel loads the correct model architecture based on the checkpoint. </details>

6. What are the techniques to be aware of when batching sequences of different lengths together?
A) Truncating
B) Returning tensors
C) Padding
D) Attention masking
<details><summary>Reveal Answer</summary> A, C, D. Truncating, Padding, and Attention Masking are all used to manage sequences of different lengths. </details>

7. What is the point of applying a SoftMax function to the logits output by a sequence classification model?
A) It softens the logits so that they're more reliable.
B) It applies a lower and upper bound so that they're understandable.
C) The total sum of the output is then 1, resulting in a possible probabilistic interpretation.
<details><summary>Reveal Answer</summary> B, C. SoftMax both bounds outputs between 0 and 1 and ensures their sum equals 1 for probabilistic interpretation. </details>

8. What method is most of the tokenizer API centered around?
A) encode, as it can encode text into IDs and IDs into predictions
B) Calling the tokenizer object directly
C) pad
D) tokenize
<details><summary>Reveal Answer</summary> B. The tokenizer's `__call__` method handles encoding and decoding inputs flexibly. </details>