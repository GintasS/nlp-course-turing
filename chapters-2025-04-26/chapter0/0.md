# Hugging Face LLM Course

Welcome, Turing College learners! 

We are thrilled that you have chosen to embark on this journey into the fascinating world of Natural Language Processing (NLP). 

This course is designed to equip you with the knowledge and skills needed to excel in NLP using the powerful tools and libraries.

This course is provided by Hugging Face. If by this time you are not sure what Hugging Face, lets explain it!

## What is Hugging Face?

Hugging Face is a company and open-source community that provides a suite of tools and libraries for Natural Language Processing (NLP) and Machine Learning (ML). 

They are best known for their Transformers library, which offers pre-trained models for a wide range of NLP tasks such as text classification, named entity recognition, question answering, and more. 

Hugging Face aims to make state-of-the-art NLP accessible to everyone by providing easy-to-use APIs and extensive documentation.

## Course Structure

As per 2025 April, the course has 13 chapters:

0. Setup
1. Transformer Models
2. Using Transformers
3. Fine-Tuning a Pre-trained model
4. Sharing Models and Tokenizers
5. The Hugging Face Datasets Library
6. The Hugging Face Tokenizers Library
7. Classical NLP Tasks
8. How to Ask for Help
9. Building and Sharing Demos
10. Curate High-Quality Datasets
11. Fine-tune Large-Language-Models
12. Build Reasoning Models
Extra: Course Events

## Turing College's recommended chapters for learners who are short on time

0. Setup
1. Transformer Models
2. Using Transformers
3. Fine-Tuning a Pre-trained model
4. Sharing Models and Tokenizers
7. Classical NLP Tasks
11. Fine-tune Large-Language-Models
12. Build Reasoning Models

## Chapter explanation

### 0. Setup

In this chapter, we will guide you through the initial setup required to get started with the course. This includes setting up your development environment, installing necessary libraries, and ensuring you have access to the datasets and tools needed for the exercises.

You can either use Google Colab, which provides free access to GPUs and a pre-configured environment, or set up a local environment on your machine. We recommend Google Colab for its ease of use and accessibility.

If you are using Google Colab, most of the required libraries are pre-installed. However, if you are setting up a local environment, you will need to install some libraries.

### 1. Transformer Models

This is the chapter where the real work begins. In this chapter, you will learn how transformers work, their capabilities, and the encoder-decoder sequence-to-sequence (seq2seq) architecture. We will also discuss the biases and limitations of transformer models. We also touch briefly on the rise of LLMs and most importantly: **What is NLP?**

### 2. Using Transformers

In this chapter, we will delve into the practical aspects of using transformers. You will learn how to load pre-trained models and tokenizers, and how to use them for various NLP tasks. We will cover the following topics:

- Loading pre-trained models and tokenizers from the Hugging Face library
- Understanding the role of tokenizers in processing text data
- Working with multiple sequences and handling different input formats
- Building basic pipelines for tasks such as sentiment analysis

### 3. Fine-Tuning a Pre-trained model

In this chapter, we will explore the process of fine-tuning pre-trained models to adapt them to specific tasks or datasets. Fine-tuning allows you to leverage the power of large, pre-trained models and customize them for your unique needs, improving performance on your specific tasks.

We will cover the following topics:
- Understanding the concept of fine-tuning and its benefits
- Preparing your dataset for fine-tuning
- Setting up the training environment and configuring hyperparameters
- Fine-tuning a model using the Hugging Face library
- Evaluating the performance of your fine-tuned model
- Tips and best practices for effective fine-tuning

### 4. Sharing Models and Tokenizers

This chapter is unique and covers the Hugging Face ecosystem. Here, you will learn how to share your models with the community, enabling others to benefit from your work and collaborate on improving models.

Here are the topics:
- Understanding the Hugging Face Model Hub and its benefits
- Preparing your model for sharing.
- Uploading your model to the Hugging Face Model Hub.
- Managing versions and updates of your shared models.
- Best practices for documenting and maintaining your shared models.
- Exploring community-contributed models and leveraging them in your projects.

### 5. The Hugging Face Datasets Library

In this chapter, we will explore the Hugging Face Datasets library, a powerful tool for accessing and working with a wide variety of datasets. The Datasets library provides an easy-to-use interface for loading, processing, and sharing datasets, making it an essential resource for any NLP practitioner.

Topics:
- Introduction to the Hugging Face Datasets library and its features
- Loading datasets from the Hugging Face Hub
- Processing and transforming datasets for your specific needs
- Creating and sharing your own datasets with the community
- Semantic search with FAISS

### 6. The Hugging Face Tokenizers Library

In this chapter, you will discover how to create a new tokenizer from scratch using a text corpus, which can then be utilized to pretrain a language model. This will be achieved with the help of the ü§ó Tokenizers library, known for providing the ‚Äúfast‚Äù tokenizers in the ü§ó Transformers library. We will examine the features offered by this library and understand how the fast tokenizers differ from the ‚Äúslow‚Äù ones.

Topics we will cover include:
- Training a new tokenizer similar to the one used by a specific checkpoint on a new text corpus
- Unique features of fast tokenizers
- Differences between the three primary subword tokenization algorithms used in NLP today
- Building a tokenizer from scratch with the ü§ó Tokenizers library and training it on a dataset

### 7. Classical NLP Tasks

In this chapter, we will delve into some of the most frequent and classical NLP tasks. These tasks form the foundation of many natural language processing applications and are essential for understanding and building more complex models.

Topics (not all) that would be covered:
- Token classification: Assigning predefined categories to text data with Named Entity Recognition (NER) and other models.
- Sentiment analysis: Determining the sentiment expressed in a piece of text
- Machine translation: Translating text from one language to another
- Text summarization: Generating concise summaries of longer texts
- Question answering: Building systems that can answer questions based on a given context

### 8. How to Ask for Help

In this chapter, we will discuss various ways to ask for help and debug issues you might encounter while working with the Hugging Face libraries and tools. Effective debugging and knowing where to seek assistance are crucial skills for any developer.

Topics covered:
- Utilizing the Hugging Face forums and community for support
- Best practices for debugging code and resolving errors
- Leveraging online resources and documentation
- Reporting issues and contributing to the Hugging Face repositories
- Collaborating with peers and mentors for problem-solving

### 9. Building and Sharing Demos

In this chapter, we will explore how to create interactive demos for your machine learning models.

Why should you build a demo or a graphical user interface (GUI) for your machine learning model? Demos offer several benefits:
- Machine learning developers can showcase their work to a broader audience, including non-technical teams or clients.
- Researchers can more easily replicate machine learning models and their behaviors.
- Quality testers or end users can identify and debug model failure points more effectively.
- Diverse users can uncover algorithmic biases in models.

### 10. Curate High-Quality Datasets

In this chapter, you will learn mostly about Argilla UI, a powerful tool for managing and annotating datasets. Argilla is designed to streamline the process of creating high-quality datasets by providing an intuitive user interface for dataset annotation and management.

Example topics:
- Setting up your own Argilla instance.
- Loading and configuring a dataset for various popular NLP tasks.
- Using the Argilla UI to annotate your dataset.
- Exporting your curated dataset to the Hub.


### 11. Fine-tune Large-Language-Models

This chapter will be divided into four sections:

1. Chat Templates
Chat templates help structure interactions between users and AI models, ensuring responses are consistent and contextually appropriate. They include elements such as system prompts and role-based messages.

2. Supervised Fine-Tuning
Supervised Fine-Tuning (SFT) is essential for adapting pre-trained language models to specific tasks. It involves training the model on a task-specific dataset with labeled examples. For a comprehensive guide on SFT, including key steps and best practices, refer to the supervised fine-tuning section of the TRL documentation.

3. Low Rank Adaptation (LoRA)
Low Rank Adaptation (LoRA) is a method for fine-tuning language models by incorporating low-rank matrices into the model‚Äôs layers. This technique allows for efficient fine-tuning while maintaining the model‚Äôs pre-trained knowledge. One of the main advantages of LoRA is the significant memory savings it provides, enabling the fine-tuning of large models on hardware with limited resources.

4. Evaluation
Evaluation is a vital part of the fine-tuning process, allowing us to assess the model‚Äôs performance on a task-specific dataset.

### 12. Build Reasoning Models

In this chapter, we will focus on explaining the open-source version of DeepSeek, reinforcement learning, and large language models (LLMs).

## How to approach this course

A good way to start by the chapter 0: setup, where you decided on the environment you're going to use.
Google Colab is sufficient for this course. But feel free to do it locally as well.

In this course, you will encounter images, diagrams, code snippets and quizes. Make sure you spend
enough time to understand them. 


**It is recommended to run the code from the labs, tinker with it, adapt it.

Also, the course is being updated almost weekly/monthly, but you can still encounter errors in the code.
Use your debugging power and or even ChatGPT to fix some of the errors. Just remember to not overuse it and first think
for yourself, and if you are stuck for an hour or so, use that.**

At the end of each of the chapter, there is a quiz, it is a good way to test your knowledge.

If you encounter some unexpected behavour, make sure to check their GitHub repository:
https://github.com/huggingface/course.

You can also explore `issues` section, if you are having problems: https://github.com/huggingface/course/issues

## FAQ from the authors

Here are some answers to frequently asked questions:
- Q: Does taking this course lead to a certification? 

  A: Currently we do not have any certification for this course. However, we are working on a certification program for the Hugging Face ecosystem ‚Äî stay tuned!

- Q: How much time should I spend on this course? 

  A: Each chapter in this course is designed to be completed in 1 week, with approximately 6-8 hours of work per week. However, you can take as much time as you need to complete the course.

- Q: Where can I ask a question if I have one? 

  A: If you have a question about any section of the course, just click on the ‚ÄùAsk a question‚Äù banner at the top of the page to be automatically redirected to the right section of the Hugging Face forums:

- Where can I get the code for the course? 

  A: For each section, click on the banner at the top of the page to run the code in either Google Colab or Amazon SageMaker Studio Lab:


## Resources

- [Official Course Link](https://huggingface.co/learn/llm-course/chapter1/1)
- [Hugging Face Forums](https://discuss.huggingface.co/)
- [Course Notebooks](https://github.com/huggingface/notebooks)

## License

This course is released under the Apache 2 license.
It is originally from https://huggingface.co/learn/llm-course/chapter1/1.
We have made various refactorings to the course to adjust more to Turing College style.