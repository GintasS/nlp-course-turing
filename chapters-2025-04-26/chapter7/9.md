

<!-- DISABLE-FRONTMATTER-SECTIONS -->

# End-of-chapter quiz



Let's test what you learned in this chapter!

### 1. Which of the following tasks can be framed as a token classification problem?

A. Sentiment analysis
B. Named entity recognition
C. Text summarization
D. Machine translation

<details><summary>Reveal Answer</summary>B</details>

### 2. What part of the preprocessing for token classification differs from the other preprocessing pipelines?

A. Tokenization
B. Label alignment
C. Data augmentation
D. Normalization

<details><summary>Reveal Answer</summary>B</details>

### 3. What problem arises when we tokenize the words in a token classification problem and want to label the tokens?

A. Tokens may not align with labels
B. Tokens are too long
C. Tokens are too short
D. Tokens are not unique

<details><summary>Reveal Answer</summary>A</details>

### 4. What does "domain adaptation" mean?

A. Adapting a model to a new language
B. Adapting a model to a new task
C. Adapting a model to a new domain
D. Adapting a model to a new dataset

<details><summary>Reveal Answer</summary>C</details>

### 5. What are the labels in a masked language modeling problem?

A. The original words
B. The masked words
C. The predicted words
D. The context words

<details><summary>Reveal Answer</summary>B</details>

### 6. Which of these tasks can be seen as a sequence-to-sequence problem?

A. Text classification
B. Named entity recognition
C. Text summarization
D. Sentiment analysis

<details><summary>Reveal Answer</summary>C</details>

### 7. What is the proper way to preprocess the data for a sequence-to-sequence problem?

A. Tokenize the input only
B. Tokenize the output only
C. Tokenize both input and output
D. Do not tokenize

<details><summary>Reveal Answer</summary>C</details>

<details><summary><span style="font-size: 1.5rem;">PyTorch Version (Click On me)</span></summary>


### 8. Why is there a specific subclass of `Trainer` for sequence-to-sequence problems?

A. To handle different input and output lengths
B. To handle different input and output formats
C. To handle different input and output tokenizations
D. To handle different input and output encodings

<details><summary>Reveal Answer</summary>A</details>

</details><br><details><summary><span style="font-size: 1.5rem;">TensorFlow Version (Click On me)</span></summary>


### 9. Why is it often unnecessary to specify a loss when calling `compile()` on a Transformer model?

A. The model automatically selects a loss
B. The loss is predefined
C. The loss is not needed
D. The loss is irrelevant

<details><summary>Reveal Answer</summary>B</details>


</details><br>

### 10. When should you pretrain a new model?

A. When you have a new task
B. When you have a new dataset
C. When you have a new architecture
D. When you have a new language

<details><summary>Reveal Answer</summary>D</details>

### 11. Why is it easy to pretrain a language model on lots and lots of texts?

A. Texts are abundant
B. Texts are easy to process
C. Texts are easy to label
D. Texts are easy to tokenize

<details><summary>Reveal Answer</summary>A</details>

### 12. What are the main challenges when preprocessing data for a question answering task?

A. Tokenizing the context
B. Tokenizing the question
C. Aligning the context and question
D. All of the above

<details><summary>Reveal Answer</summary>D</details>

### 13. How is post-processing usually done in question answering?

A. By selecting the best answer
B. By ranking the answers
C. By filtering the answers
D. By validating the answers

<details><summary>Reveal Answer</summary>A</details>
